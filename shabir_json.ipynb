{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shabirdewji/DHT/blob/master/shabir_json.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h20UXF6soESH",
        "outputId": "d18115ae-88d3-492f-fd9d-7fbe73c0fc65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON FILE SHABIR LOADED\n",
            "Start talking with the bot (type quit to stop)!\n",
            "You: Mary\n",
            "Maryam\n",
            "You: who was the ex-wife\n",
            "Mary\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "import numpy as np\n",
        "# import tflearn\n",
        "# import tensorflow\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "with open(\"shabir.json\") as file:\n",
        "    data = json.load(file)\n",
        "    print('JSON FILE SHABIR LOADED')\n",
        "# try:\n",
        "#     with open(\"data.pickle\", \"rb\") as f:\n",
        "#         words, labels, training, output = pickle.load(f)\n",
        "# except:\n",
        "    words  = []\n",
        "    labels = []\n",
        "    docs_x = []\n",
        "    docs_y = []\n",
        "    for intent in data[\"intents\"]:\n",
        "        for pattern in intent[\"patterns\"]:\n",
        "            wrds = nltk.word_tokenize(pattern)\n",
        "            words.extend(wrds)\n",
        "            docs_x.append(wrds)\n",
        "            docs_y.append(intent[\"tag\"])\n",
        "\n",
        "        if intent[\"tag\"] not in labels:\n",
        "            labels.append(intent[\"tag\"])\n",
        "\n",
        "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "    words = sorted(list(set(words)))\n",
        "\n",
        "    labels = sorted(labels)\n",
        "\n",
        "    training = []\n",
        "    output = []\n",
        "\n",
        "    out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "    for x, doc in enumerate(docs_x):\n",
        "        bag = []\n",
        "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "        for w in words:\n",
        "            if w in wrds:\n",
        "                bag.append(1)\n",
        "            else:\n",
        "                bag.append(0)\n",
        "                \n",
        "        output_row = out_empty[:]\n",
        "        output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "        training.append(bag)\n",
        "        output.append(output_row)\n",
        "        # print(\"\\n\",w,\"============training========\")\n",
        "        # print(training)\n",
        "        # print(\"\\n\",w,\"============output========\")\n",
        "        # print(output)\n",
        "\n",
        "    # training = numpy.array(training)\n",
        "    \n",
        "    # print(\"\\n\",w,\"============training after numpy========\")\n",
        "    # print(training)\n",
        "    \n",
        "    output = np.array(output)\n",
        "    # print(\"\\n\",w,\"============output after numpy========\")\n",
        "    # print(output)\n",
        "    \n",
        "    # with open(\"data.pickle\", \"wb\") as f:\n",
        "    #     pickle.dump((words, labels, training, output), f)\n",
        "    \n",
        "def bag_of_words(s, words):\n",
        "    bag = [0 for _ in range(len(words))]\n",
        "    s_words = nltk.word_tokenize(s)\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "    for se in s_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == se:\n",
        "                bag[i] = 1\n",
        "    return bag\n",
        "\n",
        "def chat():\n",
        "    print(\"Start talking with the bot (type quit to stop)!\")\n",
        "    while True:\n",
        "        inp = input(\"You: \")\n",
        "        # inp = 'where have you lived'\n",
        "        # print('input = ',inp)\n",
        "        if inp.lower() == \"quit\":\n",
        "            break\n",
        "        # results = model.predict([bag_of_words(inp, words)])\n",
        "        results = ([bag_of_words(inp, words)])\n",
        "        # print(\"\\n\",words,\"============results========\")\n",
        "        # print(results)\n",
        "        # bag = results \n",
        "        i=0\n",
        "        bg = [0 for _ in range(len(words))]\n",
        "        # print('bg ',bg)\n",
        "        \n",
        "        for myList in [results]:\n",
        "            for item in myList:\n",
        "                for itt in item:\n",
        "                    bg[i] = itt\n",
        "                    # print('bg',i)\n",
        "                    i += 1\n",
        "        # print('bg ',bg)\n",
        "        \n",
        "        no_intents = 0\n",
        "        no_patterns = 0\n",
        "        for intent in data[\"intents\"]:\n",
        "            for pattern in intent[\"patterns\"]:\n",
        "                no_patterns += 1\n",
        "            no_intents += 1\n",
        "            \n",
        "        score=[]\n",
        "        w=0\n",
        "        w1=no_patterns\n",
        "        while w < w1 :\n",
        "            score.append(0)\n",
        "            w += 1\n",
        "        # print('number of data intents', no_intents)\n",
        "        # print('number of data pattern', no_patterns)  \n",
        "        # print('score',score)\n",
        "        # print(\"\\n\",\"==========training========\")\n",
        "        k=0\n",
        "        for t in training:\n",
        "            j=0\n",
        "            for i in t:\n",
        "                # print('t:',t,'i',i,'j',j)\n",
        "                if bg[j]==i  and i==1:\n",
        "                    score[k] += 1\n",
        "                j += 1\n",
        "            k += 1\n",
        "\n",
        "#data[\"intents\"] has  all the info\n",
        "#data intents {'tag': 'citys', 'patterns': ['where have you lived', 'cities lived'], 'responses': ['I have lived in Mombasa, Nairobi, Glasgow, London, Torornto, Regina and Calgary']}\n",
        "#data intents {'tag': 'zachary', 'patterns': ['who is My son', \"son's name\"], 'responses': ['zachary']}          \n",
        "\n",
        "        # for tg in data[\"intents\"]:\n",
        "        #     print('data intents',tg )\n",
        "        \n",
        "        # print('score',score)\n",
        "            \n",
        "        results_index = np.argmax(score)\n",
        "#this returns the index of the highest score\n",
        "#docs_x [['where', 'have', 'you', 'lived'], ['cities', 'lived'], ['who', 'is', 'My', 'son'], ['son', \"'s\", 'name']]\n",
        "#docs_y ['citys', 'citys', 'zachary', 'zachary']\n",
        "        tag = docs_y[results_index]\n",
        "        tag2= docs_x[results_index]\n",
        "        sc = score[results_index]\n",
        "        # print('results_index: ',results_index)\n",
        "        # print('labels',labels)\n",
        "\n",
        "        # print('Picked Phrase: ',tag2, 'Picked tag: ',tag,'Picked score: ',sc)\n",
        "        \n",
        "#the labels are the tags\n",
        "        for tg in data[\"intents\"]:\n",
        "            if tg['tag'] == tag:\n",
        "                responses = tg['responses']\n",
        "        print(random.choice(responses))\n",
        "\n",
        "chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L9HLLvfpgXr"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "shabir.json",
      "provenance": [],
      "authorship_tag": "ABX9TyOi+3ZS018zLA3Ovf2T4U13",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}